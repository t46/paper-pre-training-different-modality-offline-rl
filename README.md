# On the Effect of Pre-training for Transformer in Different Modality on Offline Reinforcement Learning
This repository contains the LaTeX source and assets for *"On the Effect of Pre-training for Transformer in Different Modality on Offline Reinforcement Learning"*. Code for experiments and analyses is available at [https://github.com/t46/pre-training-different-modality-offline-rl](https://github.com/t46/pre-training-different-modality-offline-rl).

The original version of this paper was [published in NeurIPS 202]((https://openreview.net/pdf?id=9GXoMs__ckJ)). We released the LaTex source for this version as [neurips-2022](https://github.com/t46/paper-pre-training-different-modality-offline-rl/releases/tag/neurips-2022). Therefore, if you would like to get the source at the time of publication, please refer to the repository tagged with [neurips-2022](https://github.com/t46/paper-pre-training-different-modality-offline-rl/tree/neurips-2022).

## Citation
```
@inproceedings{
takagi2022on,
title={On the Effect of Pre-training for Transformer in Different Modality on Offline Reinforcement Learning},
author={Shiro Takagi},
booktitle={Thirty-Sixth Conference on Neural Information Processing Systems},
year={2022},
url={https://openreview.net/forum?id=9GXoMs__ckJ}
}
```
