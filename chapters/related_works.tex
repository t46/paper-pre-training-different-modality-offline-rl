\section{Related Works}
\label{section:related-works}

The study of transferability of neural representation has been one of the most important research topics in machine learning research. A bunch of studies have investigated how pre-training influences the downstream tasks, such as computer vision tasks \cite{yosinski2014transferable,djolonga2021robustness,ding2021analyzing,orhand2021quantification} and NLP tasks \cite{mou2016transferable,liu-etal-2019-linguistic,rogers2020primer}. Recently, several studies have investigated the transferability of Transformers to various NLP tasks since understanding its high transferability is of great interest. Previous research reveals that Transformer representation encodes some syntax \cite{tenney2018what}, semantics \cite{ettinger2020bert}, cross-lingual information \cite{artetxe-etal-2020-cross}, and world knowledge \cite{petroni-etal-2019-language}.
Also, it has been pointed out that Transformers obtains not only language-specific information but also more abstract inter-token dependencies \cite{ri2022pretraining,chiang2021transferability,chi-etal-2020-finding}. This finding suggests that pre-trained Transformers may apply to a wider variety of tasks than just NLP tasks.

In recent years, studies have emerged that apply the Transformer to tasks other than NLP tasks, reporting the versatility of the Transformer even in such multi-modal cases. Such prior works have shown that language-pre-trained Transformers can efficiently learn image classification \cite{lu2021pretrained}, symbolic mathematics \cite{noorbakhsh2021pretrained}, simple arithmetic \cite{brown2020language}, reinforcement learning tasks \cite{li2022pre,huang2022language,tam2022semantic,reid2022can}, and more generally, sequence modeling problems \cite{lu2021pretrained}. Our study is in line with the research that examines the applicability of pre-trained Transformers on reinforcement learning.

Most prior works applying pre-trained Transformer on reinforcement learning explicitly express goals, observations, or actions in a language \cite{yao2020keep,huang2022language,tam2022semantic}. On the other hand, the work of \cite{reid2022can} applies pre-trained Transformers on the offline RL tasks that language is unrelated to (e.g. Mujoco \cite{todorov2012mujoco}) and still reports that pre-training is helpful. This study is interesting because it suggests that language-pre-trained Transformers seem to exploit some similarity between the structure of natural language data and trajectory data, which is more abstract and fundamental than linguistic information. 
Also, exploring the applicability of pre-trained Transformers, which have already been successful, is of great engineering significance since pre-training in offline RL has room for improvement compared to other modalities. Examining the internals of Transformers, we further delve into the findings from this study by Reid et al. \cite{reid2022can} and explores the applicability of Transformers to offline RL.
