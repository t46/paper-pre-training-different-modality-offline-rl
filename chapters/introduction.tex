\section{Introduction}
\label{section:introduction}

The past few years have witnessed the tremendous success of pre-trained Transformer-based models \cite{vaswani2017attention} on versatile natural language processing (NLP) tasks. This is because pre-training with a large corpus enables Transformer-based models to gain rich universal language representations transferable to vast downstream tasks \cite{devlin2018bert,radford2019language,brown2020language}. Remarkably, recent studies demonstrate that Transformers pre-trained with language data can efficiently solve even non-NLP tasks \cite{lu2021pretrained,noorbakhsh2021pretrained,li2022pre,huang2022language,reid2022can,tam2022semantic}. Because of its versatility, it has been suggested that Transformers pre-trained with a large corpus could be called a kind of \textit{universal computation engines} \cite{lu2021pretrained}. 

One area worthwhile attempting to study the benefits of pre-trained Transformer is reinforcement learning because the applicability of pre-training to this area still has room to be explored \cite{singh2020parrot,yang2021representation,stooke2021decoupling}. In this vein of research, Reid et al. studied the effect of pre-training of Transformers on offline reinforcement learning (offline RL) tasks \cite{reid2022can}. Their study demonstrates a surprising contrast that pre-training with image significantly deteriorates performance, while that with language data does not negatively affect downstream tasks, rather it even improves performance for some datasets. 

However, it is not yet known how the presence or absence of pre-training and its content leads to differences in the performance of downstream tasks. What information does/doesn't the model pre-trained on language data (\textit{language-pre-trained model}) leverage to solve downstream tasks, and why does the model pre-trained on image data (\textit{image-pre-trained model}) result in catastrophic performance? In this paper, we approach these questions by analyzing the inside of the Transformer that is fine-tuning to Mujoco \cite{todorov2012mujoco} offline RL tasks. Analyzing the internals of Transformer, we study how randomly initialized, language-pre-trained, and image-pre-trained models acquire different representations, and what the differences could be attributed to. Our contributions are as follows:

\begin{itemize}
    \item We study the internal representation of pre-trained and randomly initialized Transformers and find that the pre-trained models largely change their representation, while they encode less information of data during fine-tuning.
    \item The analysis of parameter change and gradient shows that the pre-trained models do not change parameters much perhaps due to gradient confusion and that the performance of the image-pre-trained model might be partly affected by gradient clipping on the large gradient.
    \item We examine context-dependence by fine-tuning models with giving no context information, finding that the language-pre-trained Transformer efficiently learns even without context and this could be the case by replacing just a single Transformer block with a pre-trained one.
    \item Subsequent analysis of the change in how far away the model process the tokens of input supports the possibility that some contextual equivalent information has been positively transferred from language pre-training.
\end{itemize}