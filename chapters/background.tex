\section{Background}
\label{section:background}

\subsection{Transformer}
\label{subsection:transformer}
Transformers are sequence-to-sequence models composed of stacked identical blocks, called \textit{Transformer blocks}. Each block consists of self-attention \cite{vaswani2017attention}, multi-layer perceptron, skip-connection \cite{he2016deep}, and layer normalization (layer norm) \cite{ba2016layer}. Denoting the linear projection of input sequence of length $n$ to query, key, values of length $m$ by $Q \in \mathbb{R}^{n \times d_Q}$, $K \in \mathbb{R}^{m \times d_K}$, and $V \in \mathbb{R}^{m \times d_V}$, respectively, the self-attention is $\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^{\top}}{\sqrt{d_K}} \right) V$. We use GPTs \cite{radford2018improving}, and hence the autoregressive language model, where queries head to only keys before their position.

\subsection{Offline Reinforcement Learning and Decision Transformer}
\label{subsection:decision-transformer}
Reinforcement learning is the problem to learn to make optimal decisions in a dynamic environment. For ease of mathematical handling, the environment is usually described as a Markov decision process (MDP). The MDP consists of states $\bm{s} \in \mathcal{S}$, actions $\bm{a} \in \mathcal{A}$, transition probability $P(\bm{s}^{\prime}|\bm{s}, \bm{a})$, and reward function $r(\bm{s}, \bm{a}): \mathcal{S} \times \mathcal{A} \to \mathbb{R}$. The interaction of the agent in the environment is represented as $N$ length trajectory, or sequence of state, action, and reward $(\bm{s}_0, \bm{a}_0, r_0, \bm{s}_1, \bm{a}_1, r_1, ..., \bm{s}_N, \bm{a}_N, r_N)$, where $\bm{s}_t$, $\bm{a}_t$, and $r_t$ are state, action, and reward at time step $t$. The goal is to find an optimal policy $\pi(\bm{a}|\bm{s})$ to maximize the expected cumulative rewards $\mathbb{E}[\sum_{t=0}^Nr_t]$ 
for the trajectory.

Offline RL aims to achieve objectives of reinforcement learning from the trajectory data collected by some policy \cite{levine2020offline}. Because offline RL is purely characterized by trajectory data, several studies have proposed formulating offline RL as a sequence modeling problem \cite{janner2021offline,chen2021decision}. Decision Transformer is a seminal work that attempts to use a causal transformer (in other words, GPT architecture or decoder of BERT \cite{devlin2018bert}) to solve such a sequence modeling problem \cite{chen2021decision}. In Decision Transformer, the input trajectory representation is $\tau = (\hat{R}_0,  \bm{s}_0, \bm{a}_0, \hat{R}_1, \bm{s}_1, \bm{a}_1, ..., \hat{R}_N,  \bm{s}_N, \bm{a}_N)$, where return-to-go $\hat{R}_t = \sum_{i = t}^{N} r_i$ is the cumulative reward from time step $t$. The previous studies \cite{reid2022can,chen2021decision} employ the above problem set up to discuss the applicability of Transformer for offline RL. We also follow these studies and use the setup for our analysis.