%%%%%%%%%%%%%%
% Main Paper %
%%%%%%%%%%%%%%


%%% LM %%%

% Transformer
@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 year = {2017}
}

% BERT
@inproceedings{devlin2018bert,
    title = {BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding},
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2019"
}

% GPT2
@article{radford2019language,
  title={Language Models Are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

% GPT3
@inproceedings{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 year = {2020}
}


%%% language --> another modality %%%

% LayerNorm fine-tuning
@article{lu2021pretrained,
  title={Pretrained Transformers as Universal Computation Engines},
  author={Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  journal={arXiv preprint arXiv:2103.05247},
  year={2021}
}

@article{noorbakhsh2021pretrained,
  title={Pretrained Language Models are Symbolic Mathematics Solvers too!},
  author={Noorbakhsh, Kimia and Sulaiman, Modar and Sharifi, Mahdi and Roy, Kallol and Jamshidi, Pooyan},
  journal={arXiv preprint arXiv:2110.03501},
  year={2021}
}

@article{li2022pre,
  title={Pre-Trained Language Models for Interactive Decision-Making},
  author={Shuang Li and
               Xavier Puig and
               Chris Paxton and
               Yilun Du and
               Clinton Wang and
               Linxi Fan and
               Tao Chen and
               De{-}An Huang and
               Ekin Aky{\"{u}}rek and
               Anima Anandkumar and
               Jacob Andreas and
               Igor Mordatch and
               Antonio Torralba and
               Yuke Zhu},
  journal={arXiv preprint arXiv:2202.01771},
  year={2022}
}

@inproceedings{huang2022language,
  title={Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={Proceedings of the 39th International Conference on Machine Learning},
  year={2022}
}

% NLP --> RL
@article{reid2022can,
  title={Can Wikipedia Help Offline Reinforcement Learning?},
  author={Reid, Machel and Yamada, Yutaro and Gu, Shixiang Shane},
  journal={arXiv preprint arXiv:2201.12122},
  year={2022}
}

@article{tam2022semantic,
  title={Semantic Exploration from Language Abstractions and Pretrained Representations},
  author={Tam, Allison C and Rabinowitz, Neil C and Lampinen, Andrew K and Roy, Nicholas A and Chan, Stephanie CY and Strouse, DJ and Wang, Jane X and Banino, Andrea and Hill, Felix},
  journal={arXiv preprint arXiv:2204.05080},
  year={2022}
}


%%% RL pre-training %%%
@inproceedings{
singh2020parrot,
title={Parrot: Data-Driven Behavioral Priors for Reinforcement Learning},
author={Avi Singh and Huihan Liu and Gaoyue Zhou and Albert Yu and Nicholas Rhinehart and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ysuv-WOFeKR}
}

@inproceedings{yang2021representation,
  title={Representation Matters: Offline Pretraining for Sequential Decision Making},
  author={Yang, Mengjiao and Nachum, Ofir},
  booktitle={Proceedings of the 38th International Conference on Machine Learning},
  year={2021},
}

@InProceedings{stooke2021decoupling,
  title = 	 {Decoupling Representation Learning from Reinforcement Learning},
  author =       {Stooke, Adam and Lee, Kimin and Abbeel, Pieter and Laskin, Michael},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021},
}



%%% RL Data %%%

% Mujoco
@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year={2012}
}


%%% Transferability %%%

@inproceedings{yosinski2014transferable,
 author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {How transferable are features in deep neural networks?},
 url = {https://proceedings.neurips.cc/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf},
 year = {2014}
}

@inproceedings{djolonga2021robustness,
  title={On robustness and transferability of convolutional neural networks},
  author={Djolonga, Josip and Yung, Jessica and Tschannen, Michael and Romijnders, Rob and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Minderer, Matthias and D'Amour, Alexander and Moldovan, Dan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}

@inproceedings{ding2021analyzing,
  title={Analyzing Deep Neural Network's Transferability via Frechet Distance},
  author={Ding, Yifan and Wang, Liqiang and Gong, Boqing},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  year={2021}
}

@article{orhand2021quantification,
  title={Quantification of the transferability of features between deep neural networks},
  author={Orhand, Romain and Khodji, Hiba and Hutt, Amarin and Jeannin-Girardon, Anne},
  journal={Procedia Computer Science},
  volume={192},
  pages={138--147},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{mou2016transferable,
    title = "How Transferable are Neural Networks in {NLP} Applications?",
    author = "Mou, Lili  and
      Meng, Zhao  and
      Yan, Rui  and
      Li, Ge  and
      Xu, Yan  and
      Zhang, Lu  and
      Jin, Zhi",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    year = "2016",
}

@inproceedings{liu-etal-2019-linguistic,
    title = "Linguistic Knowledge and Transferability of Contextual Representations",
    author = "Liu, Nelson F.  and
      Gardner, Matt  and
      Belinkov, Yonatan  and
      Peters, Matthew E.  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2019",
}

@article{rogers2020primer,
  title={A primer in bertology: What we know about how bert works},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={842--866},
  year={2020},
  publisher={MIT Press}
}


%%% Pre-trained LM's representation %%%

% contextualized LM learns better syntactic representations.
@inproceedings{
tenney2018what,
title={What do you learn from context? Probing for sentence structure in contextualized word representations},
author={Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJzSgnRcKX},
}

% semantic knowledge
@article{ettinger2020bert,
  title={What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models},
  author={Ettinger, Allyson},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={34--48},
  year={2020},
  publisher={MIT Press}
}

% mono-lingual LM --> cross-lingual transfer
@inproceedings{artetxe-etal-2020-cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
}

% world knowledge
@inproceedings{petroni-etal-2019-language,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
}

% position-aware context dependence transfers （“tokens in a sequence can be characterized by its neighbor tokens at specific positions）
@inproceedings{ri2022pretraining,
    title = "Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models",
    author = "Ri, Ryokan  and
      Tsuruoka, Yoshimasa",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2022",
}

% explicit token dependency and long range implicit dependency transfers
@inproceedings{chiang2021transferability, 
title={On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets}, 
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Chiang, Cheng-Han and Lee, Hung-yi}, 
year={2022}
}

% language-independent linguistic structures
@inproceedings{chi-etal-2020-finding,
    title = "Finding Universal Grammatical Relations in Multilingual {BERT}",
    author = "Chi, Ethan A.  and
      Hewitt, John  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
}


%%% Transformer to non-NLP

% RL
@inproceedings{yao2020keep,
    title = "Keep {CALM} and Explore: Language Models for Action Generation in Text-based Games",
    author = "Yao, Shunyu  and
      Rao, Rohan  and
      Hausknecht, Matthew  and
      Narasimhan, Karthik",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2020",
}


%%% skip connection %%%

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2016}
}


%%% layer normalization %%%

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}


%%% GPT %%%

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={OpenAI blog},
  year={2018}
}


%%% Offline RL %%%

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

% sequence prediction
@inproceedings{
janner2021offline,
title={Offline Reinforcement Learning as One Big Sequence Modeling Problem},
author={Michael Janner and Qiyang Li and Sergey Levine},
booktitle={Advances in Neural Information Processing Systems},
year={2021},
url={https://openreview.net/forum?id=wgeK563QgSw}
}

% Decision Transformer
@inproceedings{
chen2021decision,
title={Decision Transformer: Reinforcement Learning via Sequence Modeling},
author={Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
booktitle={Advances in Neural Information Processing Systems},
year={2021},
url={https://openreview.net/forum?id=a7APmM4B9d}
}


%%% Huggingface Transformers %%%
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    year = "2020",
}


%%% RL Tasks %%%

% OpenAI Gym
@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

% D4RL
@article{fu2020d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}


%%% representation similarity

% CKA, shallower similar, wider similaer
@InProceedings{kornblith2019similarity,
  title = 	 {Similarity of Neural Network Representations Revisited},
  author =       {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  year = 	 {2019},
}


@inproceedings{
Raghu2020Rapid,
title={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},
author={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgMkCEtPB}
}

% CKA to context
@inproceedings{wu-etal-2020-similarity,
    title = "Similarity Analysis of Contextual Word Representation Models",
    author = "Wu, John  and
      Belinkov, Yonatan  and
      Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Glass, James",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
}

%later is sensitive
@inproceedings{Neyshabur20,
 author = {Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {What is being transferred in transfer learning? },
 year = {2020}
}

% vision transformer
@inproceedings{raghu2021vision,
title={Do Vision Transformers See Like Convolutional Neural Networks?},
author={Maithra Raghu and Thomas Unterthiner and Simon Kornblith and Chiyuan Zhang and Alexey Dosovitskiy},
booktitle={Advances in Neural Information Processing Systems},
year={2021},
url={https://openreview.net/forum?id=Gl8FHfMVTZu}
}

@inproceedings{
ramasesh2021anatomy,
title={Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics},
author={Vinay Venkatesh Ramasesh and Ethan Dyer and Maithra Raghu},
booktitle={Proceedings of the 38th International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=LhY8QdUGSuw}
}

@inproceedings{
nguyen2021do,
title={Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth},
author={Thao Nguyen and Maithra Raghu and Simon Kornblith},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=KJNcAkY8tY4}
}

% SVCCA, shallower matters
@inproceedings{Raghu17,
	Author = {Maithra Raghu and Justin Gilmer and  Jason Yosinski and Jascha Sohl-Dickstein},
	Title = {SVCCA: Singular Vector Canonical Correlation
	Analysis for Deep Learning Dynamics and
	Interpretability},
	booktitle  = {Advances in Neural
Information Processing Systems},
	Year = {2017}
}

% shallower matters
@inproceedings{Morcos_nips18,
	Author = {Ari S. Morcos  and Maithra Raghu and Samy Bengio},
	Title = {Insights on representational similarity in neural
	networks with canonical correlation},
	booktitle  = {Advances in Neural
Information Processing Systems},
	Year = {2018}
}

% shallower matters
@inproceedings{Morcos_iclr18,
	Author = {Ari S. Morcos  and David G.T. Barrett and Neil C. Rabinowitz and Matthew Botvinick},
	Title = {On the Importance of Single Directions for Generalization},
	booktitle  = {International Conference on Learning
	Representation},
	Year = {2018}
}

% shallower transferable
@inproceedings{Raghu19,
	Author = {Maithra Raghu and Chiyuan Zhang and Jon Kleinberg and Samy Bengio},
	Title = {Transfusion: Understanding Transfer Learning for
	Medical Imaging},
	booktitle  = {Advances in Neural
Information Processing Systems},
	Year = {2019}
}

@inproceedings{voita-etal-2019-bottom,
    title = "The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives",
    author = "Voita, Elena  and
      Sennrich, Rico  and
      Titov, Ivan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
}

@inproceedings{Merchant20,
    title = "What Happens To {BERT} Embeddings During Fine-tuning?",
    author = "Merchant, Amil  and
      Rahimtoroghi, Elahe  and
      Pavlick, Ellie  and
      Tenney, Ian",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    year = "2020"
}


%%% mutual information

% information bottleneck
@inproceedings{tishby2015deep,
  title={Deep learning and the information bottleneck principle},
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 IEEE Information Theory Workshop (ITW)},
  year={2015}
}

@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}

@article{hafez2019information,
  title={Information bottleneck and its applications in deep learning},
  author={Hafez-Kolahi, Hassan and Kasaei, Shohreh},
  journal={arXiv preprint arXiv:1904.03743},
  year={2019}
}

@article{goldfeld2020information,
  title={The information bottleneck problem and its applications in machine learning},
  author={Goldfeld, Ziv and Polyanskiy, Yury},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={19--38},
  year={2020},
  publisher={IEEE}
}

@article{geiger2021information,
  title={On Information Plane Analyses of Neural Network Classifiers--A Review},
  author={Geiger, Bernhard C},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2021},
  publisher={IEEE}
}

% MINE
@InProceedings{pmlr-v80-belghazi18a,
  title = 	 {Mutual Information Neural Estimation},
  author =       {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  year = 	 {2018},
}


%%% Gradient Confusion %%%
@InProceedings{sankararaman2020impact,
  title = 	 {The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent},
  author =       {Sankararaman, Karthik Abinav and De, Soham and Xu, Zheng and Huang, W. Ronny and Goldstein, Tom},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  year = 	 {2020},
}


%%% Vision Transformer %%%
@inproceedings{
dosovitskiy2020image,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}


%%% Syntax knowledge in middle layers %%%

@inproceedings{hewitt-manning-2019-structural,
    title = "A Structural Probe for Finding Syntax in Word Representations",
    author = "Hewitt, John  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
}

@article{goldberg2019assessing,
  title={Assessing BERT's syntactic abilities},
  author={Goldberg, Yoav},
  journal={arXiv preprint arXiv:1901.05287},
  year={2019}
}


%%% Action Grammar %%%

@article{lashley1951,
  title={The problem of serial order in behavior},
  author={Lashley, K. S.},
  journal={Cerebral Mechanisms in Behavior; The Hixon Symposium},
  year={1951},
  pages = "112–-146"
}

@article{greenfield1991language,
  title={Language, tools and brain: The ontogeny and phylogeny of hierarchically organized sequential behavior},
  author={Greenfield, Patricia M},
  journal={Behavioral and Brain Sciences},
  volume={14},
  number={4},
  pages={531--551},
  year={1991},
  publisher={Cambridge University Press}
}



%%%%%%%%%%%%
% Appendix %
%%%%%%%%%%%%


%%% Soft actor-critic %%%
@InProceedings{haarnoja2018soft,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  year = 	 {2018},
}


%%% Adam %%%
@inproceedings{
KingmaB14,
title={Adam: A Method for Stochastic Optimization},
author={Diederik P. Kingma and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2015},
}

%%% Hilbert-Schmidt independence criterion %%%
@inproceedings{gretton2007kernel,
 author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon and Song, Le and Sch\"{o}lkopf, Bernhard and Smola, Alex},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {A Kernel Statistical Test of Independence},
 url = {https://proceedings.neurips.cc/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf},
 year = {2007}
}

%%% Pytorch %%%
@inproceedings{Paszke19,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 year = {2019}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The following are not cited in the paper %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%% Similarity Analysis %%%

@inproceedings{vulic-etal-2020-probing,
    title = "Probing Pretrained Language Models for Lexical Semantics",
    author = "Vuli{\'c}, Ivan  and
      Ponti, Edoardo Maria  and
      Litschko, Robert  and
      Glava{\v{s}}, Goran  and
      Korhonen, Anna",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.586",
    doi = "10.18653/v1/2020.emnlp-main.586",
    pages = "7222--7240"
}

@article{phang2021fine,
  title={Fine-tuned transformers show clusters of similar representations across layers},
  author={Phang, Jason and Liu, Haokun and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2109.08406},
  year={2021}
}

@article{wu2019emerging,
  title={Emerging cross-lingual structure in pretrained language models},
  author={Wu, Shijie and Conneau, Alexis and Li, Haoran and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.01464},
  year={2019}
}

@article{mamou2020emergence,
  title={Emergence of separable manifolds in deep language representations},
  author={Mamou, Jonathan and Le, Hang and Del Rio, Miguel and Stephenson, Cory and Tang, Hanlin and Kim, Yoon and Chung, SueYeon},
  journal={arXiv preprint arXiv:2006.01095},
  year={2020}
}

% Shallower matters
@article{Raghu16,
	Author = {Maithra Raghu and Ben Poole and Jon Kleinberg and Surya Ganguli and Jascha Sohl Dickstein},
	Title = {On the Expressive Power of Deep Neural Networks},
	journal  = {International Conference on Machine
	Learning},
	Year = {2017}
}


%%% RL

% offline-rl data analysis
@article{schweighofer2021understanding,
  title={Understanding the Effects of Dataset Characteristics on Offline Reinforcement Learning},
  author={Schweighofer, Kajetan and Hofmarcher, Markus and Dinu, Marius-Constantin and Renz, Philipp and Bitto-Nemling, Angela and Patil, Vihang and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2111.04714},
  year={2021}
}

% offline-RL
@InProceedings{fujimoto2019off,
  title = 	 {Off-Policy Deep Reinforcement Learning without Exploration},
  author =       {Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  year = 	 {2019},
}



%%% LM representation

% shallower general, deeper specific
@inproceedings{van2019does,
  title={How does bert answer questions? a layer-wise analysis of transformer representations},
  author={Van Aken, Betty and Winter, Benjamin and L{\"o}ser, Alexander and Gers, Felix A},
  booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  pages={1823--1832},
  year={2019}
}

@inproceedings{peters-etal-2019-tune,
    title = "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks",
    author = "Peters, Matthew E.  and
      Ruder, Sebastian  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4302",
    doi = "10.18653/v1/W19-4302",
    pages = "7--14",
}

@inproceedings{hao-etal-2019-visualizing,
    title = "Visualizing and Understanding the Effectiveness of {BERT}",
    author = "Hao, Yaru  and
      Dong, Li  and
      Wei, Furu  and
      Xu, Ke",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1424",
    doi = "10.18653/v1/D19-1424",
    pages = "4143--4152",
}

% visual question answering
@inproceedings{
tsimpoukelli2021multimodal,
title={Multimodal Few-Shot Learning with Frozen Language Models},
author={Maria Tsimpoukelli and Jacob Menick and Serkan Cabi and S. M. Ali Eslami and Oriol Vinyals and Felix Hill},
booktitle={Advances in Neural Information Processing Systems},
year={2021},
url={https://openreview.net/forum?id=WtmMyno9Tq2}
}

% Shallower token and its local context
@inproceedings{lin-etal-2019-open,
    title = "Open Sesame: Getting inside {BERT}{'}s Linguistic Knowledge",
    author = "Lin, Yongjie  and
      Tan, Yi Chern  and
      Frank, Robert",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4825",
    doi = "10.18653/v1/W19-4825",
    pages = "241--253",
}

@inproceedings{voita-etal-2019-context,
    title = "Context-Aware Monolingual Repair for Neural Machine Translation",
    author = "Voita, Elena  and
      Sennrich, Rico  and
      Titov, Ivan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1081",
    doi = "10.18653/v1/D19-1081",
    pages = "877--886",
}

@inproceedings{ethayarajh-2019-contextual,
    title = "How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings",
    author = "Ethayarajh, Kawin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1006",
    doi = "10.18653/v1/D19-1006",
    pages = "55--65",
}

@inproceedings{
Brunner2020On,
title={On Identifiability in Transformers},
author={Gino Brunner and Yang Liu and Damian Pascual and Oliver Richter and Massimiliano Ciaramita and Roger Wattenhofer},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJg1f6EFDB}
}

% deeper longer dependency and semantic representation
@inproceedings{raganato-tiedemann-2018-analysis,
    title = "An Analysis of Encoder Representations in Transformer-Based Machine Translation",
    author = {Raganato, Alessandro  and
      Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5431",
    doi = "10.18653/v1/W18-5431",
    pages = "287--297",
}

@inproceedings{vig-belinkov-2019-analyzing,
    title = "Analyzing the Structure of Attention in a Transformer Language Model",
    author = "Vig, Jesse  and
      Belinkov, Yonatan",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4808",
    doi = "10.18653/v1/W19-4808",
    pages = "63--76",
}

@inproceedings{jawahar-etal-2019-bert,
    title = "What Does {BERT} Learn about the Structure of Language?",
    author = "Jawahar, Ganesh  and
      Sagot, Beno{\^\i}t  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1356",
    doi = "10.18653/v1/P19-1356",
    pages = "3651--3657",
}

% deeper context dependent
@article{ethayarajh2019contextual,
  title={How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings},
  author={Ethayarajh, Kawin},
  journal={arXiv preprint arXiv:1909.00512},
  year={2019}
}

@inproceedings{hu-etal-2021-investigating,
    title = "Investigating Transfer Learning in Multilingual Pre-trained Language Models through {C}hinese Natural Language Inference",
    author = "Hu, Hai  and
      Zhou, He  and
      Tian, Zuoyu  and
      Zhang, Yiwen  and
      Patterson, Yina  and
      Li, Yanting  and
      Nie, Yixin  and
      Richardson, Kyle",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.331",
    doi = "10.18653/v1/2021.findings-acl.331",
    pages = "3770--3785",
}

% subjecthood
@inproceedings{papadimitriou-etal-2021-deep,
    title = "Deep Subjecthood: Higher-Order Grammatical Features in Multilingual {BERT}",
    author = "Papadimitriou, Isabel  and
      Chi, Ethan A.  and
      Futrell, Richard  and
      Mahowald, Kyle",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.215",
    doi = "10.18653/v1/2021.eacl-main.215",
    pages = "2522--2532",
}

%%% Vision
@InProceedings{radford2021learning,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021},
}

@InProceedings{chen2020generative,
  title = 	 {Generative Pretraining From Pixels},
  author =       {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  year = 	 {2020},
}



%%% Transfer Learning

% vision-language navigation
@inproceedings{majumdar2020improving,
  title={Improving vision-and-language navigation with image-text pairs from the web},
  author={Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
  booktitle={European Conference on Computer Vision},
  pages={259--274},
  year={2020},
  organization={Springer}
}

@inproceedings{fried2018speaker,
 author = {Fried, Daniel and Hu, Ronghang and Cirik, Volkan and Rohrbach, Anna and Andreas, Jacob and Morency, Louis-Philippe and Berg-Kirkpatrick, Taylor and Saenko, Kate and Klein, Dan and Darrell, Trevor},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Speaker-Follower Models for Vision-and-Language Navigation},
 url = {https://proceedings.neurips.cc/paper/2018/file/6a81681a7af700c6385d36577ebec359-Paper.pdf},
 year = {2018}
}

@article{suglia2021embodied,
  title={Embodied bert: A transformer model for embodied, language-guided visual task completion},
  author={Suglia, Alessandro and Gao, Qiaozi and Thomason, Jesse and Thattai, Govind and Sukhatme, Gaurav},
  journal={arXiv preprint arXiv:2108.04927},
  year={2021}
}

% instruction following
@article{zhang2021hierarchical,
  title={Hierarchical task learning from language instructions with unified transformers and self-monitoring},
  author={Zhang, Yichi and Chai, Joyce},
  journal={arXiv preprint arXiv:2106.03427},
  year={2021}
}

@article{hill2020human,
  title={Human instruction-following with deep reinforcement learning via transfer-learning from text},
  author={Hill, Felix and Mokra, Sona and Wong, Nathaniel and Harley, Tim},
  journal={arXiv preprint arXiv:2005.09382},
  year={2020}
}


%%% data structure

@article{udden2020hierarchical,
  title={Hierarchical structure in sequence processing: How to measure it and determine its neural implementation},
  author={Udd{\'e}n, Julia and de Jesus Dias Martins, Mauricio and Zuidema, Willem and Tecumseh Fitch, W},
  journal={Topics in cognitive science},
  volume={12},
  number={3},
  pages={910--924},
  year={2020},
  publisher={Wiley Online Library}
}